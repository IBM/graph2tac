serialized_optimizer:
  class_name: Adam
  config:
    amsgrad: false
    beta_1: 0.9
    beta_2: 0.999
    decay: 0.0
    epsilon: 1.0e-07
    learning_rate:
      class_name: ExponentialDecay
      config:
        decay_rate: 0.99
        decay_steps: .inf
        initial_learning_rate: 0.001
        name: null
        staircase: false
    name: Adam
l2_regularization_coefficient: 1.0e-05
definition_loss_coefficient: 500.0
definition_loss_schedule: null
recompute_embeddings_between_epochs: false
max_to_keep: 1
keep_checkpoint_every_n_hours: null
qsaving: null
